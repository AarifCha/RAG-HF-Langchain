{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AarifCha/RAG-HF-Langchain/blob/main/GGUF_Model_with_Langchain_LlamaCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Dependencies"
      ],
      "metadata": {
        "id": "C60vPRgbygeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This might take a few minutes!\n",
        "!pip install -q langchain langchain_community\n",
        "!pip install -q llama-cpp-python"
      ],
      "metadata": {
        "id": "qeuGVxAndBNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64575a73-7fc9-417a-f40e-7ae1cd7c242f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the model to Google Drive"
      ],
      "metadata": {
        "id": "UK8qOBnxytck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCYovdmShPMa",
        "outputId": "d39a1197-fb5a-41cc-e4e0-0ca5275edc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# First we mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# The followling line downloads the model's gguf file from huggingface. You can also download it manually and upload it to your Google Drive.\n",
        "!huggingface-cli download bartowski/UNA-ThePitbull-21.4B-v2-GGUF --include \"UNA-ThePitbull-21.4B-v2-Q6_K.gguf\" --local-dir /content/drive/MyDrive/Colab_Notebooks/NLP_Projects/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Model"
      ],
      "metadata": {
        "id": "l9OtD6vOy2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load a Llama model and port it to C++ for faster inference."
      ],
      "metadata": {
        "id": "MhszBUV56kaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IGNORE THIS: Doesn't seem to work well with colab (to potentially use the free GPU they provide)\n",
        "# !set FORCE_CMAKE=1\n",
        "# !set CMAKE_ARGS=-DLLAMA_CUBLAS=ON\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=ON\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "id": "-HaceEUw-eoX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L_g0kLwDTPfj"
      },
      "outputs": [],
      "source": [
        "READER_MODEL_PATH = \"/content/drive/MyDrive/Colab_Notebooks/NLP_Projects/UNA-ThePitbull-21.4B-v2-Q6_K.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "\n",
        "# We can simply load the model using langchain's LlamaCpp. LlamaCpp esentially\n",
        "# ports the models to C++ so the inference can be made faster.\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# This will take almost 5 minutes!\n",
        "llm = LlamaCpp(\n",
        "    model_path=READER_MODEL_PATH,\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    n_batch=512,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvKmqWCnbPTH",
        "outputId": "4c5fd080-6ece-49b8-9207-b04488254d26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 30 key-value pairs and 471 tensors from /content/drive/MyDrive/Colab_Notebooks/NLP_Projects/UNA-ThePitbull-21.4B-v2-Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = UNA-ThePitbull-21.4B-v2\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 52\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 6144\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 48\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 92544\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,92544]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,92544]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,92544]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models/UNA-ThePitbull-21.4B-v2-GGUF/...\n",
            "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
            "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 364\n",
            "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 136\n",
            "llama_model_loader: - type  f32:  105 tensors\n",
            "llama_model_loader: - type q6_K:  366 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.5532 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 92544\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 6144\n",
            "llm_load_print_meta: n_layer          = 52\n",
            "llm_load_print_meta: n_head           = 48\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 6\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 21.42 B\n",
            "llm_load_print_meta: model size       = 16.37 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = UNA-ThePitbull-21.4B-v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 384\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors:        CPU buffer size = 16760.21 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   104.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  104.00 MiB, K (f16):   52.00 MiB, V (f16):   52.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.35 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   192.75 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1670\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '364', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '136', 'quantize.imatrix.file': '/models/UNA-ThePitbull-21.4B-v2-GGUF/UNA-ThePitbull-21.4B-v2.imatrix', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '32768', 'general.name': 'UNA-ThePitbull-21.4B-v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '6144', 'llama.feed_forward_length': '16384', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '48', 'llama.block_count': '52', 'llama.attention.head_count_kv': '8', 'general.file_type': '18', 'llama.vocab_size': '92544', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: </s>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Model"
      ],
      "metadata": {
        "id": "_3Pml2Sf-aZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just create a prompt and test our model! Since this is run on CPU, expect it to take a long time for you to get a result."
      ],
      "metadata": {
        "id": "OSbgae92aOJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n",
        "final_propmpt = prompt.format(system_prompt=\"Answer the question asked and only what is asked.\", prompt=\"What is 2+2?\")\n",
        "llm.invoke(final_propmpt)\n",
        "# Because of the higher temperature, expect to get different outputs."
      ],
      "metadata": {
        "id": "wSAJY2KTc75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "2970de62-2a59-41b9-b3d3-95a81ba5f68c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The result of 2 + 2 is 4."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =  164302.08 ms\n",
            "llama_print_timings:      sample time =      25.56 ms /    14 runs   (    1.83 ms per token,   547.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =  164301.94 ms /    53 tokens ( 3100.04 ms per token,     0.32 tokens per second)\n",
            "llama_print_timings:        eval time =  984532.46 ms /    13 runs   (75733.27 ms per token,     0.01 tokens per second)\n",
            "llama_print_timings:       total time = 1149037.47 ms /    66 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe result of 2 + 2 is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZMroBNq1BxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjqXMCRAxU/Moi4ZtnQZl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}