{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV3GRCPYSakZtG6ZDjLxVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AarifCha/RAG-HF-Langchain/blob/main/Gemini_RAG_for_Scientific_Papers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini RAG for scientific papers.\n",
        "\n",
        "In this notebook, we setup a RAG for scientific papers. The Gemini LLM (LMM) is used since it has a free tier that is ideal for application development. The prompt-to-answer pipeline is constructed using LangChain due to its flexibility. We also use LlamaParse (the gpt4o version) to convert pdf files into text. We use this perticular pdf parser due to equations and other complex structures in scientific papers. Regular pdf readers such as pypdf aren't able to properly process the documents resulting in poor performancing RAGs. Chroma vector store is paired with this LlamaIndex library to create the index."
      ],
      "metadata": {
        "id": "M2UfdlAgIZwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Keys\n",
        "\n",
        "We need to setup our API keys for both Google's Gemini and LlamaCloud. These can be accesed through these links:\n",
        "https://aistudio.google.com/app/apikey\n",
        "\n",
        "https://cloud.llamaindex.ai/api-key"
      ],
      "metadata": {
        "id": "xa3jRj_EJ2DC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTWTGKwG-8Cu",
        "outputId": "af6972d1-7990-4cc8-a988-4c8e348ff05f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n",
            "Enter your Llama Cloud API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"Enter your Llama Cloud API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to pip install these libraries to work in google colab"
      ],
      "metadata": {
        "id": "OoaR19-AKdRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q google-generativeai\n",
        "%pip install -q langchain-google-genai\n",
        "%pip install -q llama-index-embeddings-langchain\n",
        "%pip install -q langchain\n",
        "%pip install -q langchain-community\n",
        "%pip install -q tiktoken\n",
        "%pip install -q chromadb\n",
        "%pip install -q pypdf\n",
        "%pip install -q llama-index-vector-stores-chroma\n",
        "%pip install -q llama-index\n",
        "%pip install -q llama-index-embeddings-gemini\n",
        "%pip install -q llama-index-core llama-parse llama-index-readers-file python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI16Ih7xTIYH",
        "outputId": "5e6ff5cc-deff-43cb-cf49-74a558cce091"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.4/187.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.0.0 requires google-generativeai<0.8.0,>=0.7.0, but you have google-generativeai 0.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Gemini\n",
        "\n",
        "We set up three llm models to be used in our langchain chain."
      ],
      "metadata": {
        "id": "Q9BeOfhmKkil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# This llm is used on the inital prompt and chat history.\n",
        "llm_prompt = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=2000,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# This llm is used to help with document/node retrival.\n",
        "llm_context = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.4,\n",
        "    max_tokens=1500,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# This is the final llm used for answering the question asked.\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=10000,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "VXyF1ulK_wRJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the model on a question. We will also use display and Markdown to print the response neatly."
      ],
      "metadata": {
        "id": "VsRZQnkXLRhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You assist in answering questions for the question asked.\",\n",
        "    ),\n",
        "    (\"human\", \"How do multimodal models work?\"),\n",
        "]\n",
        "\n",
        "ai_msg = llm_prompt.invoke(messages)\n",
        "\n",
        "display(Markdown(ai_msg.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "vRA8Tkq5ABPd",
        "outputId": "458f4aeb-fc6d-4fab-d6e3-7bca4d7cd143",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Multimodal models are a fascinating area of AI that are revolutionizing how we interact with information. Here's a breakdown of how they work:\n\n**1. Combining Multiple Data Sources:**\n\n* **The Key:** Multimodal models excel by processing and understanding information from multiple sources like text, images, audio, and even video. This allows them to grasp a more complete picture of the world.\n* **Example:** Imagine a model analyzing a recipe. It can read the text instructions, see the ingredients in an image, and even hear the chef's voice explaining the steps. This multi-sensory input gives it a much richer understanding than a model that only reads text.\n\n**2. Representation Learning:**\n\n* **The Challenge:**  The different data types (text, images, etc.) have different structures and require different processing techniques.\n* **The Solution:** Multimodal models use techniques like **embedding** to represent each data type in a common, shared space. This allows them to compare and relate information from different sources.\n* **Example:**  Text and images can be converted into numerical vectors (embeddings) that capture their meaning. The model can then compare these vectors to understand how the text and image relate to each other.\n\n**3. Fusion and Interaction:**\n\n* **The Goal:**  Once the data is represented in a common space, the model needs to combine and interact with it.\n* **The Methods:**  Various techniques are used for fusion, including:\n    * **Early Fusion:** Combining data at the input level (e.g., concatenating image features with text features).\n    * **Late Fusion:** Combining data at the output level (e.g., using separate models for each modality and then combining their predictions).\n    * **Intermediate Fusion:** Combining data at different stages of processing.\n* **The Outcome:**  The model learns to understand the relationships between different modalities and how they contribute to the overall meaning.\n\n**4. Applications:**\n\n* **Image Captioning:** Generating descriptions for images.\n* **Video Understanding:** Analyzing and understanding the content of videos.\n* **Machine Translation:** Translating text while considering the context of images or videos.\n* **Question Answering:** Answering questions that require understanding both text and images.\n* **Medical Diagnosis:** Analyzing medical images and patient records to assist in diagnosis.\n\n**In a Nutshell:**\n\nMultimodal models are like having multiple senses working together. They learn to understand the world by combining information from different sources, representing it in a common language, and then using that information to make predictions or generate outputs. This ability to integrate multiple data types makes them incredibly powerful for a wide range of applications.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing PDF using LlamaParse\n",
        "\n",
        "We will use the LlamaCloud API to parse our PDF file using openai-GPT4o model. If your document is mostly text, maybe using the default setting might be beneficial since it is much cheaper.\n",
        "\n",
        "We first mount the google drive."
      ],
      "metadata": {
        "id": "924eOM5kMQ6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXDRrewlYIrH",
        "outputId": "455181de-2721-4015-af6c-f9d893d952f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the drive is mounted, lets parse a file from the drive."
      ],
      "metadata": {
        "id": "qyILi4dOOmF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# set up parser\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "#    fast_mode=True, # Uncomment this to use fast parser that does not use any OCR (cheapest)\n",
        "    use_vendor_multimodal_model=True,            # Comment these two lines out\n",
        "    vendor_multimodal_model_name=\"openai-gpt4o\", # if you want to use the default parser.\n",
        "    parsing_instruction=\"The following is a research paper. Write the equations in latex format.\"\n",
        ")\n",
        "\n",
        "file_extractor = {\".pdf\": parser}\n",
        "file_path = \"/content/drive/MyDrive/ML_Colab_Notebooks/NLP_Projects/Test_Docs/GANs.pdf\"\n",
        "documents = SimpleDirectoryReader(input_files=[file_path], file_extractor=file_extractor).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uduP4c8REVVh",
        "outputId": "11c79c33-bcc9-4f90-ac2c-ceb1b66c031d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id df0fa807-d4a4-4532-b49b-65cdcc9bcb12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our document converted into text (Markdown format). We need to parse it into chunks. To do so we will use Langchain's Recursive Character Text Splitter to split the documents (pages) into nodes (chunks)."
      ],
      "metadata": {
        "id": "ON2pyuU5PArj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from llama_index.core.node_parser import LangchainNodeParser\n",
        "\n",
        "# Characters used to determine where to split in order.\n",
        "SEPARATORS = [\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \".\",\n",
        "    \"?\",\n",
        "    \"!\",\n",
        "    \",\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=3000,\n",
        "    chunk_overlap=300,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=SEPARATORS,\n",
        ")\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    doc.metadata[\"page_number\"] = \"page \" + str(i)\n",
        "\n",
        "# Create a Node parser for LlamaIndex using langchain splitter.\n",
        "text_parser = LangchainNodeParser(text_splitter)\n",
        "nodes = text_parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "cVSXlb1EAs09"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nodes can now be embedded into vectors and a vector store database (db) using chroma can be created. We can also use previously created/saved vectore store to create our index."
      ],
      "metadata": {
        "id": "rtvnlLXlPkuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "import chromadb\n",
        "\n",
        "gemini_embeddings = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
        "\n",
        "persist_directory = '/content/drive/MyDrive/ML_Colab_Notebooks/NLP_Projects/chroma/' # Replace with your desired directory path\n",
        "# !rm -r /content/drive/MyDrive/ML_Colab_Notebooks/NLP_Projects/chroma/*   # Uncomment to remove old database files\n",
        "\n",
        "db = chromadb.PersistentClient(path=persist_directory)\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Comment this line and ...\n",
        "index = VectorStoreIndex(nodes, vector_store=vector_store, embed_model = gemini_embeddings)\n",
        "\n",
        "# ... Uncomment these lines to use a previously created vector store instead.\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store,\n",
        "#     embed_model=gemini_embeddings\n",
        "# )"
      ],
      "metadata": {
        "id": "KyCwoHAkTmL8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test and see how our vector store performs. We will again use display to print out the equations and nicely :D"
      ],
      "metadata": {
        "id": "MYA1pshAQSu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=3,\n",
        "    mode = \"cosine\"\n",
        ")\n",
        "\n",
        "ret_nodes = retriever.retrieve(\"What is Algorithm 1 about?\")\n",
        "print(len(ret_nodes))\n",
        "res = \"\\n\".join([i.get_content() + \"\\n\\n\" + i.metadata[\"file_name\"] +\" - \" + i.metadata[\"page_number\"] +\n",
        "                 \"\\n\\n-----------------------------------------------------------------\\n\\n\" for i in ret_nodes])\n",
        "\n",
        "\n",
        "# The parsed documents have \\[ \\] and \\( \\) instead of $$ and $ respecitivly. So we need to format it to have proper outputs.\n",
        "import re\n",
        "\n",
        "res = re.sub(r'\\\\([\\[\\]()])', lambda m: '$$' if m.group(1) in '[]' else '$', res)\n",
        "display(Markdown(f\"<b>{res}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2eekKwky23eu",
        "outputId": "877264c5-bac4-4ad1-be27-daa86bbcd6b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>$$\nC(G) = \\max_D V(G, D)\n$$\n\n$$\n= \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D_G^*(x)] + \\mathbb{E}_{x \\sim p_g} [\\log(1 - D_G^*(x))]\n$$\n\n$$\n= \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ \\log \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} \\right] + \\mathbb{E}_{x \\sim p_g} \\left[ \\log \\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)} \\right] \\tag{4}\n$$\n\nGANs.pdf - page 4\n\n-----------------------------------------------------------------\n\n\nIn other words, $ D $ and $ G $ play the following two-player minimax game with value function $ V(G, D) $:\n\n$$\n\\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x})} [\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}(\\mathbf{z})} [\\log(1 - D(G(\\mathbf{z})))].\n$$\n\n(1)\n\nIn the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as $ G $ and $ D $ are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing $ D $ to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between $ k $ steps of optimizing $ D $ and one step of optimizing $ G $. This results in $ D $ being maintained near its optimal solution, so long as $ G $ changes slowly enough. This strategy is analogous to the way that SML/PCD [31, 29] training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1.\n\nIn practice, equation 1 may not provide sufficient gradient for $ G $ to learn well. Early in learning, when $ G $ is poor, $ D $ can reject samples with high confidence because they are clearly different from the training data. In this case, $\\log(1 - D(G(\\mathbf{z})))$ saturates. Rather than training $ G $ to minimize $\\log(1 - D(G(\\mathbf{z})))$ we can train $ G $ to maximize $\\log(D(G(\\mathbf{z})))$. This objective function results in the same fixed point of the dynamics of $ G $ and $ D $ but provides much stronger gradients early in learning.\n\n![Figure 1](https://example.com/figure1.png)\n\nGANs.pdf - page 3\n\n-----------------------------------------------------------------\n\n\n**Algorithm 1** Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, $ k $, is a hyperparameter. We used $ k = 1 $, the least expensive option, in our experiments.\n\nfor number of training iterations do\n\n&nbsp;&nbsp;&nbsp;&nbsp;for $ k $ steps do\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $ m $ noise samples $\\{z^{(1)}, \\ldots, z^{(m)}\\}$ from noise prior $ p_g(z) $.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $ m $ examples $\\{x^{(1)}, \\ldots, x^{(m)}\\}$ from data generating distribution $ p_{\\text{data}}(x) $.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Update the discriminator by ascending its stochastic gradient:\n\n$$\n\\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^{m} \\left[ \\log D \\left( x^{(i)} \\right) + \\log \\left( 1 - D \\left( G \\left( z^{(i)} \\right) \\right) \\right) \\right].\n$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;end for\n\n&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $ m $ noise samples $\\{z^{(1)}, \\ldots, z^{(m)}\\}$ from noise prior $ p_g(z) $.\n\n&nbsp;&nbsp;&nbsp;&nbsp;• Update the generator by descending its stochastic gradient:\n\n$$\n\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^{m} \\log \\left( 1 - D \\left( G \\left( z^{(i)} \\right) \\right) \\right).\n$$\n\nend for\n\nThe gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\n\n## 4.1 Global Optimality of $ p_g = p_{\\text{data}} $\n\nWe first consider the optimal discriminator $ D $ for any given generator $ G $.\n\n**Proposition 1.** For $ G $ fixed, the optimal discriminator $ D $ is\n\n$$\nD_G^*(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} \\tag{2}\n$$\n\n**Proof.** The training criterion for the discriminator $ D $, given any generator $ G $, is to maximize the quantity $ V(G, D) $\n\n$$\nV(G, D) = \\int_x p_{\\text{data}}(x) \\log D(x) dx + \\int_x p_g(z) \\log(1 - D(g(z))) dz\n$$\n\n$$\n= \\int_x p_{\\text{data}}(x) \\log(D(x)) + p_g(x) \\log(1 - D(x)) dx \\tag{3}\n$$\n\nFor any $ (a, b) \\in \\mathbb{R}^2 \\setminus \\{0, 0\\} $, the function $ y \\to a \\log(y) + b \\log(1 - y) $ achieves its maximum in $[0, 1]$ at $ \\frac{a}{a+b} $. The discriminator does not need to be defined outside of $ \\text{Supp}(p_{\\text{data}}) \\cup \\text{Supp}(p_g) $, concluding the proof.\n\nNote that the training objective for $ D $ can be interpreted as maximizing the log-likelihood for estimating the conditional probability $ P(Y = y|x) $, where $ Y $ indicates whether $ x $ comes from $ p_{\\text{data}} $ (with $ y = 1 $) or from $ p_g $ (with $ y = 0 $). The minimax game in Eq. 1 can now be reformulated as:\n\n$$\nC(G) = \\max_D V(G, D)\n$$\n\n$$\n= \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D_G^*(x)] + \\mathbb{E}_{x \\sim p_g} [\\log(1 - D_G^*(x))]\n$$\n\nGANs.pdf - page 4\n\n-----------------------------------------------------------------\n\n</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we expected the outputs look really good! You can try to use pypdf (Langchain: PyPDFLoader) and see what the results look like to compare. They will be no where as good!"
      ],
      "metadata": {
        "id": "uHMXQMSdRp5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Chain\n",
        "\n",
        "Now we will set up a not so basic Langchain chain. We will use function calling, output/structure parsing and prompt engeering. Lets go through this slowly."
      ],
      "metadata": {
        "id": "m1CuZEw9R967"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import tool\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "\n",
        "# First we create a prompt to determine if we need any context. We correct the user query,\n",
        "# add a set of topics to help with retrival using the chat history. Read through the system\n",
        "# prompt to understand what this first model call is used for. Note that we use a positive\n",
        "# example (what to do) in order to help the model. Negative examples (what not to do) are not\n",
        "# recommended.\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a research assistant that helps the user with understanding topics and \"\n",
        "    \"finding information relavant to what they are asking. You will use the get_context function \"\n",
        "    \"when necessary to retrive the information you need to answer the question. If the question \"\n",
        "    \"does not require an calls such as 'hello, how are you?', pass an empty string as the 'query' parameter to the get_context function. \"\n",
        "    \"The user might ask a question based on the previous response. In such cases, you have been given the chat history \"\n",
        "    \"to rephrase the question with missing details to have a better retrival of relavant documents. \"\n",
        "    \"Make sure the edits are as minimal as possible, such as replacing this, that etc.\"\n",
        "    \"The chat history is surrounded by [[[...]]] and is given in a reverse order, meaning the latest output is at the top and the oldest at the bottom. \"\n",
        "    \"The user and assistant messages in the history are labeled with a prefix h_, so h_user and h_assistant.\"\n",
        "    \"Hence when asked about the previous question or response, use the top chat history to rephrase the question. \"\n",
        "    \"If the question contains all the necessary information and does not rely on previous chats use the \"\n",
        "    \"unaltered version of the question to call the get_context function. If there is a chat history relavant to the \"\n",
        "    \"question asked, such as when the user asks for more details about the previous question, create a string of topics from that chat history. \"\n",
        "    \"First have a thought, then output the question to be used for the function, and then finally the function call. \"\n",
        "    \"Output the thought, question, and function parameter in a json format. Always set the k value to 2. \"\n",
        "    \"If the query parameter is set to an empty string, set the value of the Question key as just the inputted question. \"\n",
        "    \"If nothing is relavant in the chat history to the question created put an empty string for topics and do not try to make up or forcefully fit in topics. \"\n",
        "    \"Do not repeat these topics in the rephrased question neither add the topic in the question to the string of topics.\\n\\n\"\n",
        "    \"Example: user: What can you use that model used for?\\n\"\n",
        "    \"Chat History:[[[\\nh_assistant: A generative adversarial network (GAN) is a deep learning architecture. \"\n",
        "    \"It trains two neural networks to compete against each other to generate more authentic new data from a given training dataset.\\n\"\n",
        "    \"h_user: what is a GAN?]]]\\n\\n\"\n",
        "    \"\"\"model:\n",
        "    \"Thought\": \"The question asks about uses of 'that model'. This is not an independent question hence it is\n",
        "important to modify this question. From the chat history I can tell that the 'that model' corresponds to GANs/ Generative Adversarial Networks.\n",
        "Hence I should replace the question 'What can you use a Generative Adversarial Network (GAN) for?' From the chat history I can see\n",
        "that the topics 'Deep Learning' and 'Neural Networks' might be helpful.\",\n",
        "    \"Question\": \"What can you use a Generative Adversarial Network (GAN) for?\",\n",
        "    \"Function\": \"get_context\",\n",
        "    \"Parameters\":\n",
        "        \"query\": \"What can you use a Generative Adversarial Network (GAN) for?\",\n",
        "        \"topics\": \"Deep Learning, Neural Networks\",\n",
        "        \"k\": 2\n",
        "    \"\"\"),\n",
        "    (\"user\", \"{input}\\nChat History: [[[{chat_history}]]]\"),\n",
        "    ])\n",
        "\n",
        "\n",
        "# Now we set up a pydantic class to help define our tool and parameters for the\n",
        "# model to call. This function returns context to be used by the final model.\n",
        "# We will also use a model call within the get_context window (unless the query is\n",
        "# an empty string) to help retrive more information.\n",
        "\n",
        "class get_context_pydantic(BaseModel):\n",
        "    query: str = Field(description=\"The query to search for.\")\n",
        "    topics: str = Field(description=\"The topics to search for.\")\n",
        "    k: int = Field(description=\"The number of documents to retrieve for each topic.\")\n",
        "\n",
        "@tool(args_schema=get_context_pydantic)\n",
        "def get_context(query: str, topics: str, k: int = 2):\n",
        "    \"\"\"\n",
        "    This function gets k most relevant documents for a given query\n",
        "    and concatenates their content into a single string.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query to search for.\n",
        "        topics (str): The topics to search for.\n",
        "        k (int): The number of documents to retrieve for each topic.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the concatenated content of the retrieved documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # This helps reduce the model calling by 1 when no context is needed.\n",
        "    if query == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # Again make sure to read this prompt to understand what this model call is supposed to do.\n",
        "    message = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a research assistant that helps elaborate the question asked by the user by asking more questions. \"\n",
        "            \"Suggest five additional related questions to help them find the information they need, for the provided question. \"\n",
        "            \"You are also given a list of topics, create \"+str(k)+\" questions for each topic as well. \"\n",
        "            \"Try to make those questions be related to the provided question if possible.\"\n",
        "            \"Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topics. \"\n",
        "            \"Make sure they are complete questions, and that they are related to the original question. \"\n",
        "            \"Output one question per line. Do not number the questions. Do not output anything other than the questions.\"\n",
        "        ),\\\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Question: \"+query+\"\\n\"+\"Topics: \"+topics\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    multiple_query = [query] + llm_prompt.invoke(message).content.split(\"\\n\")[:-1]\n",
        "\n",
        "    unique_documents = set()\n",
        "    retriever = VectorIndexRetriever(\n",
        "        index=index,\n",
        "        similarity_top_k=k,\n",
        "        mode=\"cosine\",\n",
        "        )\n",
        "\n",
        "    for q in multiple_query:\n",
        "        nodes = retriever.retrieve(q)\n",
        "\n",
        "        for i,node in enumerate(nodes):\n",
        "            unique_documents.add(node.get_content()+\"\\n\"+\"Metadata: \"\n",
        "                                    +str(node.metadata.get(\"file_name\")) +\" - \"\n",
        "                                    +str(node.metadata.get(\"page_number\"))+\"\\n\\n\")\n",
        "\n",
        "    context = \"\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {i}: \"+doc\n",
        "         for i, doc in enumerate(unique_documents)])\n",
        "\n",
        "    return context\n",
        "\n",
        "# Now that we have the function created, we will attach it to the model that determines how to call this function.\n",
        "llm_prompt.bind(functions=[get_context], tool_choice=\"get_context\")\n",
        "\n",
        "\n",
        "# We also need another function that uses the information/context to answer the question asked.\n",
        "# We restrict it to answer only based on the context give and also give reference to where\n",
        "# that information was retrieved to make sure we are getting correct information.\n",
        "\n",
        "class QA_answer_pydantic(BaseModel):\n",
        "    query: str = Field(description=\"The question to be answered\")\n",
        "    doc_context: str = Field(description=\"The context to use for the answering the question\")\n",
        "\n",
        "@tool(args_schema=QA_answer_pydantic)\n",
        "def QA_answer(query: str, doc_context: str):\n",
        "    \"\"\"\n",
        "    This function returns an answer to a query using the QA LLM pipeline and context\n",
        "    provided by the get_context method.\n",
        "\n",
        "    Args:\n",
        "        query (str): The question to be answered.\n",
        "        doc_context (str): The context to use for the answering the question.\n",
        "    Returns:\n",
        "        str: The answer to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"\"\"You are a research assistant that answers a question asked by the user based only on the documents given to you.\n",
        "Explain with as much detail as possible to help the user with their research. Make sure the answer are elaborate and have a lot of detail.\n",
        "If the user is having a simple chat such as \"Hey!, How are you doing?\", just repond normally. But if it is a question about some topic do not answer unless the documents contain the answer.\n",
        "If the question was answered using the documents, use the metadata to give the page number and name of the book or paper used to answer the question.\n",
        "If multiple books were relavant in answering the question list them all. Make sure to add\n",
        "references but only using the metadata and not make up references. If there are many pages from the same book or pdf, reference the book/pdf once but with all the pages listed.\n",
        "Format the response in a markdown (.md) format but keep the equations in latex format. Do not use ``` in the markdown text.\"\"\",\n",
        "        # We tell the model to not use ``` since equations inside it are not printed properly.\n",
        "    ),\\\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Question: \" + query\n",
        "    ),\\\n",
        "    (\n",
        "        \"context\"\n",
        "        \"Documents:\\n\" + doc_context\n",
        "    )]\n",
        "\n",
        "    answer = llm.invoke(messages).content\n",
        "\n",
        "    return answer\n",
        "\n",
        "# The first model, llm_prompt, outputs a json formatted string of information. We need to parse this and convert\n",
        "# it into a python dictionary for function calling. We have to provide a schema of the output and\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "\n",
        "response_schemas = [ResponseSchema(name=\"Thought\", description=\"The thought process used to find the correct question to be used.\"),\n",
        "    ResponseSchema(name=\"Question\",description=\"The question to be used to find the context needed to answer that question.\"),\n",
        "    ResponseSchema(name=\"Function\",description=\"The function to be called.\"),\n",
        "    ResponseSchema(name=\"Parameters\",description=\"A dictionary of parameters that will used in the function call.\")]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# This uses the parsed dictonary to call the get_context function with the parameters\n",
        "# set by the model and passes those results as parameters to the QA_answer funciton.\n",
        "def route(result):\n",
        "    tools = {\"get_context\": get_context}\n",
        "    doc_context = tools[result[\"Function\"]].run(result[\"Parameters\"])\n",
        "    return {\"query\": result[\"Question\"], \"doc_context\": doc_context}\n",
        "\n",
        "# Now we use LCEL to chain our different parts into a pipeline\n",
        "chain = prompt | llm_prompt | output_parser | route | QA_answer"
      ],
      "metadata": {
        "id": "P7Bf46WfjWb6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets try the chain on a question."
      ],
      "metadata": {
        "id": "tiWLsQkvV3rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"Can you tell me about algorithm 1 and also write it out?\"\n",
        "result_of_q = chain.invoke({\"input\":Question, \"chat_history\":\"\"})\n",
        "res_formatted = re.sub(r'\\\\([\\[\\]()])', lambda m: '$$' if m.group(1) in '[]' else '$', result_of_q)\n",
        "display(Markdown(res_formatted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "3c7nQFBSBcYm",
        "outputId": "9b99ac81-9933-4e70-deb2-46a56d92c7bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Algorithm 1: Minibatch stochastic gradient descent training of generative adversarial nets.** The number of steps to apply to the discriminator, $k$, is a hyperparameter. We used $k = 1$, the least expensive option, in our experiments.\n\n**for** number of training iterations **do**\n\n&nbsp;&nbsp;&nbsp;&nbsp;**for** $k$ steps **do**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $m$ noise samples $\\{z^{(1)}, \\ldots, z^{(m)}\\}$ from noise prior $p_g(z)$.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $m$ examples $\\{x^{(1)}, \\ldots, x^{(m)}\\}$ from data generating distribution $p_{\\text{data}}(x)$.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Update the discriminator by ascending its stochastic gradient:\n\n$$\n\\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^{m} \\left[ \\log D \\left( x^{(i)} \\right) + \\log \\left( 1 - D \\left( G \\left( z^{(i)} \\right) \\right) \\right) \\right].\n$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;**end for**\n\n&nbsp;&nbsp;&nbsp;&nbsp;• Sample minibatch of $m$ noise samples $\\{z^{(1)}, \\ldots, z^{(m)}\\}$ from noise prior $p_g(z)$.\n\n&nbsp;&nbsp;&nbsp;&nbsp;• Update the generator by descending its stochastic gradient:\n\n$$\n\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^{m} \\log \\left( 1 - D \\left( G \\left( z^{(i)} \\right) \\right) \\right).\n$$\n\n**end for**\n\nThe gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\n\nThe Algorithm 1 discussed above is for training generative adversarial networks. The main idea of this algorithm is to use the two networks, generator $G$ and discriminator $D$, to learn the data generating distribution. The generator $G$ tries to generate data that looks similar to the real data and the discriminator $D$ tries to distinguish between the real data and the generated data. The algorithm uses stochastic gradient descent to update the parameters of the generator and discriminator networks. The number of steps to apply to the discriminator, $k$, is a hyperparameter. The authors used $k = 1$, the least expensive option, in their experiments.\n\n**References:**\n\n[1] GANs.pdf, pp. 3, 5. \n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful! It answered the question with beautifully formatted equations and cited the pages! Lets put this response and the question into a chat history and see if it is able to answer an question that refereces to algorithm 1 indirectly."
      ],
      "metadata": {
        "id": "1aOv_oxeWB6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = \"[[[\"+\"\\nh_assistant: \"+res_formatted+\"\\n\"+\"h_user: \"+Question+\"]]]\"\n",
        "Question = \"Can you tell me about its convergence?\"\n",
        "result_of_q = chain.invoke({\"input\":Question, \"chat_history\":chat_history})\n",
        "res_formatted = re.sub(r'\\\\([\\[\\]()])', lambda m: '$$' if m.group(1) in '[]' else '$', result_of_q)\n",
        "display(Markdown(res_formatted))"
      ],
      "metadata": {
        "id": "9oe5NguWoBIU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "e9f14569-b2e4-46c2-c60c-da668f161d3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Proposition 2 in *\"Generative Adversarial Nets\"* (Goodfellow et al.) states that if the discriminator and generator networks have sufficient capacity, and during each iteration of Algorithm 1, the discriminator reaches its optimal state given the generator, and the generator is updated to improve the criterion:\n\n$$\nE_{x \\sim p_{\\text{data}}} [\\log D_G^*(x)] + E_{x \\sim p_g} [\\log(1 - D_G^*(x))],\n$$\n\nthen the generator's distribution ($p_g$) will converge to the true data distribution ($p_{\\text{data}}$).\n\n**Proof:**\n\n1. **Convexity:** The proof begins by considering the value function ($V(G, D) = U(p_g, D)$) as a function of the generator's distribution ($p_g$). It highlights that $U(p_g, D)$ is convex with respect to $p_g$.\n\n2. **Subderivatives and Gradient Descent:** The proof utilizes the property that the subderivatives of a supremum of convex functions encompass the derivative of the function at its maximum point. This implies that performing gradient descent updates for $p_g$ at the optimal discriminator ($D$) for a given generator ($G$) is equivalent to finding the subderivative of the supremum of $U(p_g, D)$.\n\n3. **Unique Global Optima:** Theorem 1 in the paper establishes that the supremum of $U(p_g, D)$ is convex in $p_g$ and has a unique global optimum when $p_g = p_{\\text{data}}$.\n\n4. **Convergence:** Given the convexity and the unique global optima, with sufficiently small updates to $p_g$ in the direction provided by the gradient descent, $p_g$ will converge to $p_{\\text{data}}$.\n\nTherefore, under the specified conditions, Algorithm 1 ensures the convergence of the generator's distribution to the true data distribution.\n\n**In simpler terms:**\n\nImagine training a counterfeiter (generator) and a detective (discriminator) to create realistic fake currency. The detective tries to distinguish real from fake, while the counterfeiter learns from the detective's feedback to improve their fakes. If the detective is good enough and the counterfeiter is constantly learning, eventually the fakes will become indistinguishable from the real currency. This convergence, where the fake distribution matches the real distribution, is what Proposition 2 guarantees under ideal conditions.\n\n**References:**\n\n- Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In *Advances in neural information processing systems* (pp. 2672-2680). (pages 3, 5) \n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was able to answer it without a problem! You should try changing these questions, and use your own documents to test how well this does. Playing around with the prompts is also encouraged to see how the results can even drastically vary when prompts are modified."
      ],
      "metadata": {
        "id": "YxdU3dKWWeDX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3z7_HJOXCxtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}